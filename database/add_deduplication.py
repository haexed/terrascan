#!/usr/bin/env python3
"""
Database Migration: Add Deduplication Constraints
Prevents duplicate environmental data from being stored
"""

import os
import psycopg2
from datetime import datetime

def add_deduplication_constraints():
    """Add unique constraints to prevent duplicate metric data"""

    print("ğŸ“‹ Terrascan Database Deduplication Migration - starting")

    database_url = os.environ.get('DATABASE_URL')
    if not database_url:
        print("âŒ DATABASE_URL not found - cannot run migration")
        return False

    print("ğŸ”Œ Connecting to database...")
    try:
        print("ğŸ”§ Adding deduplication constraints to metric_data table...")
        conn = psycopg2.connect(database_url)
        print("âœ… Database connection established")
        cursor = conn.cursor()
        print("ğŸ“‹ Database cursor created")
        
        # Check if constraint already exists
        print("ğŸ” Checking existing constraints...")
        cursor.execute("""
            SELECT constraint_name
            FROM information_schema.table_constraints
            WHERE table_name = 'metric_data'
            AND constraint_type = 'UNIQUE'
            AND constraint_name = 'unique_metric_measurement'
        """)
        print("ğŸ“‹ Constraint check query executed")
        
        result = cursor.fetchone()
        print(f"ğŸ“Š Constraint check result: {result}")
        if result:
            print("âœ… Deduplication constraint already exists")
            cursor.close()
            conn.close()
            return True
        
        # Add composite unique constraint for deduplication
        print("ğŸ“Š Creating unique constraint for metric deduplication...")
        cursor.execute("""
            ALTER TABLE metric_data
            ADD CONSTRAINT unique_metric_measurement
            UNIQUE (provider_key, metric_name, timestamp, location_lat, location_lng)
        """)
        print("âœ… Unique constraint created")
        
        # Add index for faster deduplication queries
        print("ğŸš€ Creating performance index for deduplication...")
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_metric_dedup
            ON metric_data (provider_key, metric_name, timestamp, location_lat, location_lng)
        """)
        print("âœ… Performance index created")
        
        print("ğŸ’¾ Committing changes...")
        conn.commit()
        print("âœ… Changes committed")
        cursor.close()
        conn.close()
        print("ğŸ”’ Database connection closed")

        print("âœ… Deduplication constraints added successfully!")
        print("ğŸ¯ Terrascan now prevents duplicate environmental data")
        return True
        
    except psycopg2.errors.UniqueViolation as e:
        print(f"âš ï¸ UniqueViolation caught: {e}")
        print("âš ï¸ Existing duplicate data found - cleaning required first")
        print("ğŸ’¡ Run cleanup_duplicates() before adding constraints")
        return False

    except psycopg2.OperationalError as e:
        print(f"âŒ Database connection error: {e}")
        return False

    except Exception as e:
        print(f"âŒ Unexpected migration error: {e}")
        print(f"ğŸ” Exception type: {type(e).__name__}")
        import traceback
        print(f"ğŸ“œ Stack trace: {traceback.format_exc()}")
        return False

def cleanup_existing_duplicates():
    """Remove duplicate records keeping the most recent ones"""
    
    database_url = os.environ.get('DATABASE_URL')
    if not database_url:
        print("âŒ DATABASE_URL not found")
        return False
    
    try:
        print("ğŸ§¹ Cleaning up existing duplicate records...")
        conn = psycopg2.connect(database_url)
        cursor = conn.cursor()
        
        # Get count of duplicates before cleanup
        cursor.execute("""
            SELECT COUNT(*) FROM (
                SELECT provider_key, metric_name, timestamp, location_lat, location_lng, COUNT(*)
                FROM metric_data 
                GROUP BY provider_key, metric_name, timestamp, location_lat, location_lng
                HAVING COUNT(*) > 1
            ) as duplicates
        """)
        duplicate_groups = cursor.fetchone()[0]
        
        if duplicate_groups == 0:
            print("âœ… No duplicates found - database is clean!")
            cursor.close()
            conn.close()
            return True
        
        print(f"ğŸ” Found {duplicate_groups} groups of duplicate records")
        
        # Delete duplicates, keeping the most recent (highest ID) for each group
        print("ğŸ—‘ï¸ Removing duplicate records (keeping most recent)...")
        cursor.execute("""
            DELETE FROM metric_data 
            WHERE id NOT IN (
                SELECT MAX(id) 
                FROM metric_data 
                GROUP BY provider_key, metric_name, timestamp, location_lat, location_lng
            )
        """)
        
        deleted_count = cursor.rowcount
        conn.commit()
        cursor.close()
        conn.close()
        
        print(f"âœ… Cleanup complete! Removed {deleted_count} duplicate records")
        print(f"ğŸ¯ Database is now optimized and ready for deduplication constraints")
        return True
        
    except Exception as e:
        print(f"âŒ Cleanup failed: {e}")
        return False

def get_duplicate_stats():
    """Get statistics about duplicate data"""

    print("ğŸ” get_duplicate_stats() starting...")
    database_url = os.environ.get('DATABASE_URL')
    if not database_url:
        print("âŒ No DATABASE_URL found")
        return None

    print("ğŸ”Œ Connecting to database for stats...")
    try:
        conn = psycopg2.connect(database_url, connect_timeout=10)
        print("âœ… Stats database connection established")
        cursor = conn.cursor()

        # Total records - simple count first
        print("ğŸ“Š Getting total record count...")
        cursor.execute("SELECT COUNT(*) FROM metric_data")
        total_records = cursor.fetchone()[0]
        print(f"ğŸ“‹ Total records: {total_records}")
        
        # Skip complex queries if too many records (causes timeout)
        if total_records > 100000:
            print("âš ï¸ Large dataset detected, skipping complex duplicate analysis")
            cursor.close()
            conn.close()
            return {
                'total_records': total_records,
                'unique_records': total_records,  # Assume no duplicates for large datasets
                'duplicate_records': 0,
                'duplicate_groups': 0,
                'efficiency': '100.0%'
            }

        # Unique records (what we should have) - only for smaller datasets
        print("ğŸ” Counting unique records (complex query)...")
        cursor.execute("""
            SELECT COUNT(*) FROM (
                SELECT DISTINCT provider_key, metric_name, timestamp, location_lat, location_lng
                FROM metric_data
            ) as unique_records
        """)
        unique_records = cursor.fetchone()[0]
        print(f"ğŸ“‹ Unique records: {unique_records}")
        
        # Duplicate groups - simplified
        print("ğŸ” Counting duplicate groups...")
        cursor.execute("""
            SELECT COUNT(*) FROM (
                SELECT provider_key, metric_name, timestamp, location_lat, location_lng, COUNT(*)
                FROM metric_data
                GROUP BY provider_key, metric_name, timestamp, location_lat, location_lng
                HAVING COUNT(*) > 1
            ) as duplicates
        """)
        duplicate_groups = cursor.fetchone()[0]
        print(f"ğŸ“‹ Duplicate groups: {duplicate_groups}")
        
        # Duplicate records
        duplicate_records = total_records - unique_records
        print(f"ğŸ“Š Calculated duplicate records: {duplicate_records}")

        cursor.close()
        conn.close()
        print("ğŸ”’ Stats database connection closed")
        
        return {
            'total_records': total_records,
            'unique_records': unique_records,
            'duplicate_records': duplicate_records,
            'duplicate_groups': duplicate_groups,
            'efficiency': f"{(unique_records/total_records*100):.1f}%" if total_records > 0 else "0%"
        }
        
    except psycopg2.OperationalError as e:
        print(f"âŒ Database connection timeout/error: {e}")
        return None
    except Exception as e:
        print(f"âŒ Unexpected error getting duplicate stats: {e}")
        import traceback
        print(f"ğŸ“œ Stats error traceback: {traceback.format_exc()}")
        return None
        return None

if __name__ == "__main__":
    print("ğŸš€ Terrascan Database Deduplication Migration")
    print("=" * 50)
    print("ğŸ“‹ Migration script starting...")

    # Get current duplicate statistics
    print("ğŸ” Getting database statistics...")
    stats = get_duplicate_stats()
    print(f"ğŸ“Š Statistics result: {stats is not None}")
    if stats:
        print("ğŸ“Š Current Database Statistics:")
        print(f"   Total records: {stats['total_records']:,}")
        print(f"   Unique records: {stats['unique_records']:,}")
        print(f"   Duplicate records: {stats['duplicate_records']:,}")
        print(f"   Duplicate groups: {stats['duplicate_groups']:,}")
        print(f"   Storage efficiency: {stats['efficiency']}")
        print()
        print("ğŸ” Proceeding with migration logic...")
        
        if stats['duplicate_records'] > 0:
            print("ğŸ§¹ Step 1: Clean up existing duplicates...")
            print("ğŸ” Calling cleanup function...")
            if cleanup_existing_duplicates():
                print("\nğŸ”§ Step 2: Add deduplication constraints...")
                print("ğŸ” Calling constraint function...")
                add_deduplication_constraints()
            else:
                print("âŒ Cannot proceed - cleanup failed")
        else:
            print("ğŸ”§ Adding deduplication constraints...")
            print("ğŸ” Calling constraint function directly...")
            result = add_deduplication_constraints()
            print(f"ğŸ“Š Constraint function result: {result}")
    else:
        print("âŒ Cannot connect to database - stats is None")
        print("ğŸ” Migration script ending with database connection failure") 
